# ğŸ” Insight Retriever

Insight Retriever is a lightweight, open-source **Retrieval-Augmented Generation (RAG)** system built to ingest, semantically chunk, embed, and retrieve insights from unstructured documents like **PDFs, CSVs, and TXT files**. It enables users to ask natural language questions and receive intelligent, context-aware answers sourced directly from their own documents â€” without relying on paid APIs like OpenAI.

---

## ğŸ¯ Use Cases

Insight Retriever is designed for real-world scenarios such as:

### ğŸ¢ Business & Enterprise
- Internal document search (HR, Legal, SOPs)
- Auto-answering employee FAQs
- Extracting KPIs from reports and spreadsheets

### ğŸ“ˆ Data & Research Teams
- Automated Q&A from market research PDFs
- Summarizing survey results in CSVs
- Fast retrieval of findings from technical papers

### ğŸ“ Academic & Educational
- Literature review automation
- Contextual question answering from lecture notes
- Parsing and querying research papers

---

## ğŸ§  What It Does

Insight Retriever performs the following steps:

### 1. ğŸ“‚ Ingest Documents
Supports:
- `PDF` â€“ via PyMuPDF or PDFMiner
- `CSV` â€“ via Pandas
- `TXT` â€“ plain text files

### 2. âœ‚ï¸ Semantic Chunking
Documents are split into **meaningful chunks** based on context, not arbitrary size. This improves retrieval accuracy and LLM understanding.

### 3. ğŸ“ Embedding with MiniLM
Each chunk is embedded using **MiniLM**, a small yet powerful transformer model that generates dense vector representations.

### 4. âš¡ Fast Similarity Search with FAISS
Embeddings are stored in a **FAISS index** for efficient nearest-neighbor search at scale.

### 5. ğŸ” RAG (Retrieval-Augmented Generation)
When a user asks a question:
- Related chunks are retrieved using vector similarity.
- These chunks are passed into **Flan-T5** (a lightweight open-source LLM from Google) via a **LangChain RAG pipeline**.
- The model generates a grounded, context-aware answer.

---

## ğŸ› ï¸ Tech Stack

| Component        | Tool/Library            | Purpose                                      |
|------------------|--------------------------|----------------------------------------------|
| Language Model   | Flan-T5 (HuggingFace)    | Generates final answers                      |
| Embeddings       | MiniLM                   | Converts chunks into vectors                 |
| Vector Store     | FAISS                    | Fast similarity search                       |
| Orchestration    | LangChain                | RAG pipeline setup                           |
| Deployment       | AWS SageMaker (optional) | Cloud deployment                             |
| Data Parsers     | PyMuPDF, Pandas          | Parse PDF, CSV, TXT                          |

---

## âš™ï¸ How It Works

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Upload    â”‚
      â”‚ PDF/CSV/TXTâ”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Semantic Chunk â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Embed with MiniLMâ”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
     â”‚ FAISS     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Vector DB â”‚               â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚
           â”‚                    â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
      â”‚ Retrieve â”‚â—„â”€â”€â”€â”€â”€â”€â–¶â”‚  Query   â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Generate with     â”‚
 â”‚ Flan-T5 (via RAG) â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
     â”‚  Answer   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
